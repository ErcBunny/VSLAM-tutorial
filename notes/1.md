# CH1: Framework & Math Formulation

## VSLAM Framework

1.  Sensor data acquisition: images from cameras and data from IMU and encoders
   1. Monocular Cam
      * Motion is calculated using pixel disparity
      * Suffers from scale ambiguity
   2. Stereo Cam
      * Essentially a pixel matching problem which requires GPUs for real time processing
      * The longer a baseline of the camera is, the farther it can measure
      * Depth range and accuracy are limited by baseline length and camera resolution
   3. RGBD Cam
      * Use TOF method to get depth data which is not computational expensive
      * Suffers from issues including narrow measurement range, noisy data, small field of view, susceptibility to sunlight interference
      * Unable to measure transparent material
2. Frontend and loop closing: estimate movement and generate a rough local map
   1. VO/VIO
   2. The frontend is more relevant to computer vision topics, such as image feature extraction and matching
   3. Loop closing determines whether the robot has returned to its previous position in order to reduce the accumulated drift
      * visual loop detection is essentially an algorithm for calculating similarities of images
3. Backend: applies optimization to generate a fully optimized trajectory and map (mapping/reconstruction)
   1. Mainly refers to the process of dealing with the noise in SLAM systems
   2. Relevant to the state estimation research area
   3. About mapping
      * Metrical maps emphasize the exact metrical locations of the objects in maps
        * Sparse map: landmarks for localization
        * Dense map: occupancy grids (squares/voxels) for navigation
      * Topological maps emphasize the relationships among map elements
        * Compact in size but difficult to be utilized in planning/navigation
   
   > If the working environment is limited to fixed and rigid with stable lighting conditions and no human interference, the visual SLAM problem is basically solved.

## Mathematical Formulation

1. robot state: $\bold{x}_k=f(\bold{x}_{k-1}, \bold{u}_k, \bold{w}_k)$
   * $\bold{x}_k$ is the robot state vector at time k
   * $\bold{u}_k$ is the measured state delta and $\bold{w}_k$ is measurement noise
2. landmark observation: $\bold{z}_{k, j}=h(\bold{y}_j, \bold{x}_k, \bold{v}_{k, j})$
   * Robot sees a landmark point $\bold{y}_j$ at $\bold{x}_k$ and generates an observation data $\bold{z}_{k,j}$ with the presence of measurement noise $\bold{v}_{k,j}$
   * The observation data is actually sensor output data about where the landmark is
3. Problem formulation:
   * With sensor output $\bold{u}$ and $\bold{z}$, and measurement noise $\bold{w}$ and $\bold{v}$, estimate the true value $\bold{x}$(localization) and $\bold{y}$(mapping)
   * KF->EKF->Particle Filter->Graph Optimization
